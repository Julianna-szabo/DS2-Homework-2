---
title: "DS2 Homework 2"
author: "Julianna Szabo"
date: "4/6/2021"
output: html_document
---

# Exercise 1

```{r cars}
# clear memories
rm(list = ls())

# Load libraries

library(tidyverse)
library(keras)
library(grid)
library(here)
library(grid)
library(magick)

# install.packages("tensorflow")
library(tensorflow)
# install_tensorflow()
# library(reticulate)
# install_miniconda()

my_seed <- 29032021
```

## Load data

```{r loading data}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y

class_names <- c('T-shirt/top',
                  'Trouser',
                  'Pullover',
                  'Dress',
                  'Coat', 
                  'Sandal',
                  'Shirt',
                  'Sneaker',
                  'Bag',
                  'Ankle boot')
```

I've also loaded in the labels for later displaying of images

## A, Show some example images from the data

This graph is credited to the Tension flow documentation dealing with this dataset.
```{r, display}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  img <- x_train[i, , ]
  img <- t(apply(img, 2, rev)) 
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste(class_names[y_train[i] + 1]))
}
```

You can see that the images show fashion items such as clothing or others. I have also included the tags so that we can see that the items were actually correctly indentified instead of just showing numbers between 0 and 9 as tags which we would have no real way to check easily.

## B,Train a fully connected deep network to predict items

### 1, Normalizing the data

####Scaling the axis so that they are between 0 and 1

```{r}
x_train <- array_reshape(x_train, c(dim(x_train)[1], 784)) 
x_test <- array_reshape(x_test, c(dim(x_test)[1], 784)) 

x_train <- as.matrix(x_train) / 255
x_test <- as.matrix(x_test) / 255
```

####  One-hot encoding for the outcome

```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

### Building the first model

```{r}
model_1 <- keras_model_sequential()
model_1 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>% # adding 128 nods
  layer_dropout(rate = 0.3) %>% # 30% of the set will be the same
  layer_dense(units = 10, activation = 'softmax') # 10 nods since 10 results

summary(model_1)

compile(
  model_1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# Adding the data

history_1 <- fit(
  model_1, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.3
)

history_1

plot(history_1)
```

Looks like the first one does somewhat well with validation set. Even reaching 0.88 on the first try that is a very good sign.
Let's try changing some of the parameters to get them closer together.

### Adding an additional layer



```{r}
model_2 <- keras_model_sequential()
model_2 %>%
  layer_dense(units = 250, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 128, activation = 'relu') %>% #adding another layer
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_2)

compile(
  model_2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# Add the data

history_2a <- fit(
  model_2, 
  x_train, y_train,
  epochs = 30,
  batch_size = 128,
  validation_split = 0.3
)

history_2a

plot(history_2a)
```

Now we can see that data evens out relatively quickly. This means that we reach the max accuracy after only a few iterations. The accuracy has also improved  to around 0.90 in the validation set. There are slight signs of overfitting, but nothing too mayor.

#### Let's try running this with 250 in a batch instead

```{r}
history_2b <- fit(
  model_2, 
  x_train, y_train,
  epochs = 30,
  batch_size = 250,
  validation_split = 0.3
)

history_2b

plot(history_2b)
```

While it did help increase the accuracy slightly to around 0.9, this model looks somewhat overfit to the training set. However, with that being said it still helped improve the model.

### Larger dropout rate

I have increased the dropout rate from 0.3 to 0.5 to see if that makes a difference in this model.

```{r}
model_3 <- keras_model_sequential()
model_3 %>%
  layer_dense(units = 250, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_3)

compile(
  model_3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_3 <- fit(
  model_3, 
  x_train, y_train,
  epochs = 30,
  batch_size = 128,
  validation_split = 0.3
)

history_3

plot(history_3)
```

This seems much less overfit, however it lowered the accuracy back to around 0.87-0.88 which is where we started. So this increase in dropout rate hurt out model. It might be worth trying to exact oppposite.

### Lets try changing the activation function

I changed all the activation functions to "softmax" since that is one of the other very popular activation functions for neural networks. Everything else has been kept the same in this model (back to the 0.3 dropout rate).

```{r}
model_4 <- keras_model_sequential()
model_4 %>%
  layer_dense(units = 250, activation = 'softmax', input_shape = c(784)) %>%
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 128, activation = 'softmax') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_4)

compile(
  model_4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_4 <- fit(
  model_4, 
  x_train, y_train,
  epochs = 30,
  batch_size = 128,
  validation_split = 0.3
)

history_4

plot(history_4)
```

Looks like this is actually worse than the other activation functions.The accuracy dropped to 0.7 in the validation set which is much below all the other models.  This may be due to the shape of the data that fits better to a "relu" activation function than the "softmax" one.

### Lets add one more layer

In model five I've created a more complex mode to see if it would overfit to the data. For this I added an additional layer with 250 nodes as well as lowered the dropout rate to 0.01.

```{r}
model_5 <- keras_model_sequential()
model_5 %>%
  layer_dense(units = 250, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.01) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 250, activation = 'relu') %>%  
  layer_dense(units = 10, activation = 'softmax')

summary(model_5)

compile(
  model_5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_5 <- fit(
  model_5, 
  x_train, y_train,
  epochs = 30,
  batch_size = 128,
  validation_split = 0.3
)

history_5

plot(history_5)
```

This one does great on training, but looking at the validation set, it seems overfit. The accuracy is worse than the baseline at around 0.87-0.88.
Lowering the dropout rate made the model worse as seen before as well. And adding another layer made it overfit. This shows that there is a point of complexity with neural networks like with any other model where they overfit to the data.

```{r}
summary <- data.frame(
  model1 = mean(history_1$metrics$val_accuracy),
  model2a = mean(history_2a$metrics$val_accuracy),
  model2b = mean(history_2b$metrics$val_accuracy),
  model3 = mean(history_3$metrics$val_accuracy),
  model4 = mean(history_4$metrics$val_accuracy),
  model5 = mean(history_5$metrics$val_accuracy)
)

summary
```

Comparing all the accuracies on the validation set it can be seen that Model 4 is definitely overfit to the training set. Further Model 2 performs the best, but is improved when we increse the bath size. It is interesting since it gives the best results even if it is a bit overfit.

Now let's look at Model 2 with a larger batch size on the test set.

```{r}
evaluate(model_2, x_test, y_test, batch_size = 250)
```

It is somewhat lower in the test set (0.01) but that is expected. Still very very close though. This is a very good sign with a small validation and test set compared to the training set. This means that the data is not too overfit and will most likely be good for generalization and scaling

## D, Convulated network

We need to reload the data so it is not scaled. From there we can turn it into an arrey that can be used for convulated networks.

```{r}
# Reload the data so it is not scaled
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y

# Turning it into an arrey

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

# rescale

x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

### Model building

```{r}
cnn_model_1 <- keras_model_sequential()
cnn_model_1 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_model_1)

compile(
  cnn_model_1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_6 <- fit(
  cnn_model_1, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.2
)

history_6

plot(history_6)
```

This model already shows great results (0.91), although it may be a bit overfit.
The numbers look better on the accuracy but the plot still shows that the training and validation are somewhat far apart.

#### Let's again try more layers

I've added one more layer to see if I can ge the training and validaiton results even closer togehter. This may result in an overfit model.

```{r}
cnn_model_2 <- keras_model_sequential()
cnn_model_2 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_model_2)

compile(
  cnn_model_2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_7 <- fit(
  cnn_model_2, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.2
)

history_7

plot(history_7)
```

While this validation error is lower (less than 0.9), it shows less signs of overfitting. The additional layer actually surprisingly didn't overfit but helped even it out. However, it didn't achieve as good results as other models.

#### Lets try a different activation function for one of the layers

We saw above that the "softmax" activation function didn't work out well. I wanted to see if that changes when using these convaluted models.

```{r}
cnn_model_3 <- keras_model_sequential()
cnn_model_3 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 50, activation = 'softmax') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 50, activation = 'softmax') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_model_3)

compile(
  cnn_model_3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_8 <- fit(
  cnn_model_3, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.2
)

history_8

plot(history_8)
```

That lowered the results in the train but made it much better in the validation. However, other models still do better.

#### Try running model 2 with a larger batch size

```{r}
history_9 <- fit(
  cnn_model_2, 
  x_train, y_train,
  epochs = 30, 
  batch_size = 250,
  validation_split = 0.2
)

history_9

plot(history_9)
```

This mode is not doing well, it is very overfit to the training data.

#### Changing the dropout rate

```{r}
cnn_model_4 <- keras_model_sequential()
cnn_model_4 %>%
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    activation = 'relu',
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.01) %>%
  layer_flatten() %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dropout(rate = 0.01) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(cnn_model_4)

compile(
  cnn_model_4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_10 <- fit(
  cnn_model_4, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.2
)

history_10

plot(history_10)
```

This model is also overfitting with the accuracy barely reaching 90 in the validation set.

#### Summary of all the models

```{r}
summary_2 <- data.frame(
  model6 = mean(history_6$metrics$val_accuracy),
  model7 = mean(history_7$metrics$val_accuracy),
  model8 = mean(history_8$metrics$val_accuracy),
  model9 = mean(history_9$metrics$val_accuracy),
  model10 = mean(history_10$metrics$val_accuracy)
)

summary_2
```

The best is actually model 10 even with the horrible curve

#### Checking against the test set

```{r}
evaluate(cnn_model_4, x_test, y_test)
```

This is a significant improvement over the fully connected models. This improvement can be seen both in the validation and the test set.



# Exercise 2

## A, Preprocess

```{r}
# Load the data
example_image_path <- file.path(here(), "/data/hot-dog-not-hot-dog/train/hot_dog/1000288.jpg")
image_read(example_image_path)

# A, Preprocess the data

img <- image_load(example_image_path, target_size = c(150, 150))
x <- image_to_array(img) / 255
grid::grid.raster(x)

train_datagen <- image_data_generator(rescale = 1/255) 
test_datagen <- image_data_generator(rescale = 1/255) 

image_size <- c(150, 150)
batch_size <- 50

train_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/train/"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

test_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/test/"),
  test_datagen,
  target_size = image_size,
  batch_size = batch_size,
  class_mode = "binary"
)
```


## B, Convolutional neural network

```{r}
hot_dog_model <- keras_model_sequential() 
hot_dog_model %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 8, activation = 'relu') %>% 
  layer_dense(units = 1, activation = "sigmoid")   # for binary

hot_dog_model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- hot_dog_model %>% fit_generator(
  train_generator,
  steps_per_epoch = 498 / batch_size,
  epochs = 30
)

hot_dog_model_baseline_eval <- as.data.frame(evaluate(hot_dog_model, test_generator))
```

## C, Add augmentation

```{r}
xx <- flow_images_from_data(
  array_reshape(x * 255, c(1, dim(x))),
  generator = train_datagen
)

augmented_versions <- lapply(1:10, function(ix) generator_next(xx) %>%  {.[1, , , ]})
# see examples by running in console:
grid::grid.raster(augmented_versions[[1]])

train_datagen_2 = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

hot_dog_model_2 <- keras_model_sequential() 
hot_dog_model_2 %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 8, activation = 'relu') %>% 
  layer_dense(units = 1, activation = "sigmoid")   # for binary

hot_dog_model_2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

train_generator_2 <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/train/"), # Target directory  
  train_datagen_2,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

history_2 <- hot_dog_model_2 %>% fit_generator(
  train_generator_2,
  steps_per_epoch = 498 / batch_size,
  epochs = 30
)

hot_dog_model_2_eval <- as.data.frame(evaluate(hot_dog_model_2, test_generator))
```


## D, Using a pre-training model

```{r}
model_imagenet <- application_mobilenet(weights = "imagenet")

img <- image_load(example_image_path, target_size = c(224, 224))  # 224: to conform with pre-trained network's inputs
x <- image_to_array(img)

# ensure we have a 4d tensor with single element in the batch dimension,
# the preprocess the input for prediction using mobilenet
x <- array_reshape(x, c(1, dim(x)))
x <- mobilenet_preprocess_input(x)

# make predictions then decode and print them
preds <- model_imagenet %>% predict(x)
mobilenet_decode_predictions(preds, top = 3)[[1]]

train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

image_size <- c(128, 128)
batch_size <- 100  # for speed up

train_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/train/"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images 
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)

test_generator <- flow_images_from_directory(
  file.path(here(), "data/hot-dog-not-hot-dog/test/"), # Target directory  
  test_datagen,
  target_size = image_size,
  batch_size = batch_size,
  class_mode = "binary"
)

base_model <- application_mobilenet(weights = 'imagenet', include_top = FALSE,
                                    input_shape = c(image_size, 3))


predictions <- base_model$output %>% 
  layer_global_average_pooling_2d() %>% 
  layer_dense(units = 16, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')

pre_train_model <- keras_model(inputs = base_model$input, outputs = predictions)

pre_train_model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

pre_train_model %>% fit_generator(
  train_generator,
  steps_per_epoch = 498 / batch_size,
  epochs = 1,  # takes long time to train more
)

hot_dog_model_baseline_eval <- as.data.frame(evaluate(pre_train_model, test_generator))
```
